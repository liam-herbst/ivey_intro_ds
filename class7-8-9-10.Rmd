---
title: "advertising.Rmd"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.


Read the advertising.csv data set
```{r}
#Read CSV into R

AdData <- read.csv("~/downloads/advertising.csv", header=TRUE)

#Display the first few elements of the data to make sure that data is read
head(AdData)
```

```{r}
# *scatter plot* of the data

plot(AdData$TV, AdData$sales, pch=3)
plot(AdData$radio, AdData$sales, pch = 20)
plot(AdData$newspaper, AdData$sales, pch = 20)
```

```{r}
pairs(AdData) ### Confirm that this is the proper visualization
```


Get the correlation matrix to prepare the data for regression
```{r}
#cor == correlation. The blank area before the comma below means that we take ALL rows
cor(AdData[ , 1:3])
```
We don't want to see correlations that are > 0.4 or < -0.4

We also didn't include the column 4 (sales) as this column is for y

You can round digits for clarity

```{r}
round(cor(AdData[ , 1:3]), 2)
```

Create a regression Model to predict sales

Approach 1:

```{r}
#Create Regression Model
regModel <- lm(AdData$sales ~ AdData$TV + AdData$radio + AdData$newspaper)
```

Approach 2:

```{r}
# Create Regression Model
regModel <- lm(sales ~ TV + radio + newspaper, data = AdData)
```

Approach 3:

```{r}
# Create Regression Model
regModel <- lm(sales ~ ., data = AdData)
```

The p value, R-squared and scatter plots will demonstrate whether the variables are a good fit

```{r}
summary(regModel)
```
The p value (Pr(>|t|) column above) is less than 0.05 for TV and radio but not for newspaper (0.86). This tells us that newspaper is not a good predictor and it should be removed from our model.

R-squared is a measure of accuracy. R-squared doesn't exist in more complicated ML models. Residuals are what we'll look for.
The r-squared value is 0.89 is quite high which indicates the model is accurate.


The line of best fit: y = 2.9 + 0.04TV + 0.19radio -0.001newspaper
This represents us finding values for each of the features

Residuals (see top of summary report above): the difference between the prediction and the actual data set (Residual = prediction - actual)
Residuals are a measure of model accuracy. Ideally, the min and max are as close as possible to zero. The size of the residuals is represented by the distance the data point is from the trend line.

Create a new model with eliminating newspaper

Option 1:
```{r}
# Create Regression Model
regModel <- lm(sales ~ TV + radio, data = AdData)
```

Option 2:
```{r}
# Create Regression Model
regModel <- lm(sales ~ . - newspaper, data = AdData)
summary(regModel)
```
Newspaper has been eliminated. It is very important to get rid of data that is irrelevant (noisy data)

There is not much difference at all in the model's residuals, coefficients or r-squared. 

```{r}
#Example of coefs_TV model
summary(coef(regModel_TV))
```


```{r}
# Model for one variable
regModel_TV <- lm(AdData$sales ~ AdData$TV)

# Plot data and regression equation
coefs_TV = coef(regModel_TV)
plot(AdData$TV, AdData$sales, pch=20, col='red')
abline(coefs_TV[1], coefs_TV[2])
```
[In abline, coefs_TV[1] is the intercept, and coefs_TV[2] is the sales)

There are two main problems with this graph. The lower left curve is... a curve and cannot be represented by a linear regression. The second problem is the upper right bound are consistently increasing. 

```{r}
AdData$sqrtTV <- sqrt(AdData$TV)
head(AdData)
```
```{r}
nonlinear_regModel <- lm(sales ~ sqrtTV, data = AdData)
```

```{r}
summary(nonlinear_regModel)
```

The residuals are smaller with the new model which insinuates that it is more accurate. (despite the r-squared being lower)

This model: sales = 0.99(sqrtTV) + 2.6724 #intercept
It is a linear regression model that is non-linear

```{r}
dim(AdData)
```

Get the number of data points that we have

```{r}
dim(AdData)
num_samples = dim(AdData)[1]
num_samples
```

Set the sampling rate to 80% (or any appropriate rate)

```{r}
sampling.rate = 0.8
```

Create a training set from 80% of the available data

```{r}
# Randomly select which data points will be included in the training set

training <- sample(1:num_samples, sampling.rate * num_samples, replace = FALSE)

# Note that replace = FALSE to make sure that a data point is not chosen more than once

# Training

# Define the training set with the selected training data points 

trainingSet <- subset(AdData[training, ])
head(trainingSet)
```

The testing set is the remaining data (20%)

```{r}
# The remaining data will form the testing set

testing <- setdiff(1:num_samples, training)

# Define the testing set with the selected testing data points
testingSet <- subset(AdData[testing, ])
head(testingSet)
```

The training set has 160 datapoints and the testing set has 40 datapoints

```{r}
regModel_part2 <- lm(sales ~ . - newspaper, data = trainingSet)
summary(regModel_part2)
```

Need to calculate the MSE. MSE = Residuals, Residuals = Actual - Prediction

Do predictions for the testing set

```{r}
predictions <- predict(regModel_part2, testingSet)
predictions
head(testingSet)
```
The whole number above details which row number it is in the actual data
The 6 decimal number is the sales prediction
The actual number can be found in data frame (second figure above) (match the rows!)

Get the prediction errors

```{r}
error = predictions - testingSet$sales
error
```

Calculate the Mean Square Error

```{r}
mse = mean(error^2)
mse
```
```{r}
rmse = sqrt(mse)
rmse
```
rmse is how we will evaluate models

This is not very accurate given that the numbers are so low (e.g. Sales)


```{r}
#plot
plot(testingSet$TV, testingSet$sales, pch=20, col='red')
```

```{r}
AdData <- read.csv(("~/downloads/advertising.csv"), header = TRUE)
num_samples = dim(AdData)[1]
sampling.rate = 0.8
training <- sample(1:num_samples, sampling.rate * num_samples, replace = FALSE)
trainingSet <- subset(AdData[training, ])
testing <- setdiff(1:num_samples, training)
testingSet <- subset(AdData[testing, ])
```

```{r}
library(rpart)
```

```{r}
decTreeModel <- rpart(sales ~., data= trainingSet)
```

```{r}
plot(decTreeModel, margin = 0.1)
text(decTreeModel)
```
Leaving the tree size free leads to overfitting. Tune the size by looking at the relative error and the complexity parameter

```{r}
plotcp(decTreeModel)
```

Prune the tree at a cp = 0.03

```{r}
pruned_decTreeModel = prune(decTreeModel, cp=0.03)
#Display pruned tree
plot(pruned_decTreeModel, margin = 0.1)
text(pruned_decTreeModel)
```

```{r}
plotcp(pruned_decTreeModel)
```

Evaluate the decision tree model using the testing set
```{r}
# Predictions
predictions <- predict(pruned_decTreeModel, testingSet)
#Calculate Error
error = predictions - testingSet$sales
#Calculate the MSE
mse = mean(error^2)
mse
rmse = sqrt(mse)
rmse
```

plot the actual vs predicted values (for the testing set)

```{r}
plot(testingSet$TV, testingSet$sales, pch=20, col='red')
```

Random Forest

Load the random forest library
```{r}
library(randomForest)
```

Read the data and split it

```{r}
AdData <- read.csv(("~/downloads/advertising.csv"), header = TRUE)
num_samples = dim(AdData)[1]
sampling.rate = 0.8
training <- sample(1:num_samples, sampling.rate * num_samples, replace = FALSE)
trainingSet <- subset(AdData[training, ])
testing <- setdiff(1:num_samples, training)
testingSet <- subset(AdData[testing, ])
```

Train a random forest using the training set data

```{r}
RandForestModel <- randomForest(sales ~., data = trainingSet)
plot(RandForestModel)
```

You can limit the number of trees in the forest, for example, set the limit ot 200 trees

```{r}
RandForestModel<- randomForest(sales ~ ., data = trainingSet, ntree = 200)
```

Evaluate the decision tree model using the testing set

```{r}
# Perform
predictions <- predict(RandForestModel, testingSet)

#Calc Error
error = predictions - testingSet$sales

mse =  mean(error^2)
mse
rmse = sqrt(mse)
rmse
```

Plot the actual points compared to the predictions

```{r}
plot(testingSet$TV, testingSet$sales, pch=20, col='red')
points(testingSet$TV, predictions, pch=20, col='blue')
```


You should never create a single decision tree. You should always create a random forest.